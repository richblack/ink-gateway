package services

import (
	"context"
	"semantic-text-processor/models"
	"sync"
	"time"
)

// VectorSimilarityCache provides optimized caching for vector search results
type VectorSimilarityCache struct {
	cache    map[string]*VectorSearchResult
	mutex    sync.RWMutex
	maxSize  int
	ttl      time.Duration
	lastUsed map[string]time.Time
}

// VectorSearchResult represents cached vector search results
type VectorSearchResult struct {
	Results    []models.SearchResult `json:"results"`
	TotalCount int                   `json:"total_count"`
	CachedAt   time.Time             `json:"cached_at"`
}

// NewVectorSimilarityCache creates a new vector similarity cache
func NewVectorSimilarityCache(maxSize int, ttl time.Duration) *VectorSimilarityCache {
	cache := &VectorSimilarityCache{
		cache:    make(map[string]*VectorSearchResult),
		maxSize:  maxSize,
		ttl:      ttl,
		lastUsed: make(map[string]time.Time),
	}

	// Start cleanup goroutine
	go cache.startCleanup()

	return cache
}

// Get retrieves a cached vector search result
func (c *VectorSimilarityCache) Get(key string) (*VectorSearchResult, bool) {
	c.mutex.RLock()
	defer c.mutex.RUnlock()

	result, exists := c.cache[key]
	if !exists {
		return nil, false
	}

	// Check if expired
	if time.Since(result.CachedAt) > c.ttl {
		return nil, false
	}

	// Update last used time
	c.lastUsed[key] = time.Now()

	return result, true
}

// Set stores a vector search result in cache
func (c *VectorSimilarityCache) Set(key string, result *VectorSearchResult) {
	c.mutex.Lock()
	defer c.mutex.Unlock()

	// Ensure capacity
	if len(c.cache) >= c.maxSize {
		c.evictLRU()
	}

	result.CachedAt = time.Now()
	c.cache[key] = result
	c.lastUsed[key] = time.Now()
}

// evictLRU removes the least recently used item
func (c *VectorSimilarityCache) evictLRU() {
	var oldestKey string
	var oldestTime time.Time

	for key, lastUsed := range c.lastUsed {
		if oldestKey == "" || lastUsed.Before(oldestTime) {
			oldestKey = key
			oldestTime = lastUsed
		}
	}

	if oldestKey != "" {
		delete(c.cache, oldestKey)
		delete(c.lastUsed, oldestKey)
	}
}

// startCleanup runs periodic cleanup of expired entries
func (c *VectorSimilarityCache) startCleanup() {
	ticker := time.NewTicker(5 * time.Minute)
	defer ticker.Stop()

	for range ticker.C {
		c.cleanup()
	}
}

// cleanup removes expired entries
func (c *VectorSimilarityCache) cleanup() {
	c.mutex.Lock()
	defer c.mutex.Unlock()

	now := time.Now()
	for key, result := range c.cache {
		if now.Sub(result.CachedAt) > c.ttl {
			delete(c.cache, key)
			delete(c.lastUsed, key)
		}
	}
}

// TagStatisticsCache provides optimized caching for tag statistics and frequency data
type TagStatisticsCache struct {
	stats         map[string]int
	tagCombos     map[string]int
	popularTags   []string
	mutex         sync.RWMutex
	lastRefresh   time.Time
	refreshInterval time.Duration
	maxSize       int
}

// NewTagStatisticsCache creates a new tag statistics cache
func NewTagStatisticsCache(maxSize int, refreshInterval time.Duration) *TagStatisticsCache {
	return &TagStatisticsCache{
		stats:         make(map[string]int),
		tagCombos:     make(map[string]int),
		popularTags:   make([]string, 0),
		refreshInterval: refreshInterval,
		maxSize:       maxSize,
	}
}

// GetStats retrieves tag statistics for given tags
func (c *TagStatisticsCache) GetStats(tags []string) (map[string]int, bool) {
	c.mutex.RLock()
	defer c.mutex.RUnlock()

	// Check if refresh needed
	if time.Since(c.lastRefresh) > c.refreshInterval {
		return nil, false
	}

	result := make(map[string]int)
	allFound := true

	for _, tag := range tags {
		if count, exists := c.stats[tag]; exists {
			result[tag] = count
		} else {
			allFound = false
		}
	}

	return result, allFound
}

// UpdateStats updates tag statistics
func (c *TagStatisticsCache) UpdateStats(tagStats map[string]int, popularTags []string) {
	c.mutex.Lock()
	defer c.mutex.Unlock()

	c.stats = tagStats
	c.popularTags = popularTags
	c.lastRefresh = time.Now()
}

// GetPopularTags returns the most popular tags
func (c *TagStatisticsCache) GetPopularTags(limit int) []string {
	c.mutex.RLock()
	defer c.mutex.RUnlock()

	if limit > len(c.popularTags) {
		limit = len(c.popularTags)
	}

	result := make([]string, limit)
	copy(result, c.popularTags[:limit])
	return result
}

// FullTextIndexCache provides optimized caching for full-text search results
type FullTextIndexCache struct {
	cache       map[string]*FullTextCacheEntry
	suggestions map[string][]string
	mutex       sync.RWMutex
	maxSize     int
	ttl         time.Duration
}

// FullTextCacheEntry represents cached full-text search results
type FullTextCacheEntry struct {
	Results     []models.FullTextResult `json:"results"`
	Suggestions []string                `json:"suggestions"`
	TotalCount  int                     `json:"total_count"`
	CachedAt    time.Time               `json:"cached_at"`
}

// NewFullTextIndexCache creates a new full-text index cache
func NewFullTextIndexCache(logger interface{}) *FullTextIndexCache {
	return &FullTextIndexCache{
		cache:       make(map[string]*FullTextCacheEntry),
		suggestions: make(map[string][]string),
		maxSize:     1000,
		ttl:         15 * time.Minute,
	}
}

// Get retrieves cached full-text search results
func (c *FullTextIndexCache) Get(key string) (*FullTextCacheEntry, bool) {
	c.mutex.RLock()
	defer c.mutex.RUnlock()

	entry, exists := c.cache[key]
	if !exists {
		return nil, false
	}

	// Check if expired
	if time.Since(entry.CachedAt) > c.ttl {
		return nil, false
	}

	return entry, true
}

// Set stores full-text search results in cache
func (c *FullTextIndexCache) Set(key string, entry *FullTextCacheEntry) {
	c.mutex.Lock()
	defer c.mutex.Unlock()

	// Ensure capacity
	if len(c.cache) >= c.maxSize {
		// Simple eviction - remove oldest
		var oldestKey string
		var oldestTime time.Time

		for k, v := range c.cache {
			if oldestKey == "" || v.CachedAt.Before(oldestTime) {
				oldestKey = k
				oldestTime = v.CachedAt
			}
		}

		if oldestKey != "" {
			delete(c.cache, oldestKey)
		}
	}

	entry.CachedAt = time.Now()
	c.cache[key] = entry
}

// SearchCacheAnalyzer analyzes search patterns for optimization
type SearchCacheAnalyzer struct {
	patterns     map[string]*SearchPattern
	mutex        sync.RWMutex
	analysisInterval time.Duration
}

// NewSearchCacheAnalyzer creates a new search cache analyzer
func NewSearchCacheAnalyzer(analysisInterval time.Duration) *SearchCacheAnalyzer {
	analyzer := &SearchCacheAnalyzer{
		patterns:     make(map[string]*SearchPattern),
		analysisInterval: analysisInterval,
	}

	go analyzer.startAnalysis()
	return analyzer
}

// RecordSearch records a search pattern for analysis
func (a *SearchCacheAnalyzer) RecordSearch(pattern string, duration time.Duration, cacheHit bool) {
	a.mutex.Lock()
	defer a.mutex.Unlock()

	if existing, exists := a.patterns[pattern]; exists {
		existing.Frequency++
		existing.LastUsed = time.Now()
		existing.AvgDuration = (existing.AvgDuration + duration) / 2

		// Update cache hit rate
		if cacheHit {
			existing.CacheHitRate = (existing.CacheHitRate*float64(existing.Frequency-1) + 1.0) / float64(existing.Frequency)
		} else {
			existing.CacheHitRate = (existing.CacheHitRate * float64(existing.Frequency-1)) / float64(existing.Frequency)
		}
	} else {
		hitRate := 0.0
		if cacheHit {
			hitRate = 1.0
		}

		a.patterns[pattern] = &SearchPattern{
			Pattern:     pattern,
			Frequency:   1,
			LastUsed:    time.Now(),
			AvgDuration: duration,
			CacheHitRate: hitRate,
		}
	}
}

// GetOptimizationSuggestions returns cache optimization suggestions
func (a *SearchCacheAnalyzer) GetOptimizationSuggestions() []models.CacheOptimizationSuggestion {
	a.mutex.RLock()
	defer a.mutex.RUnlock()

	suggestions := make([]models.CacheOptimizationSuggestion, 0)

	// Identify patterns with high frequency but low cache hit rate
	for pattern, stats := range a.patterns {
		if stats.Frequency > 10 && stats.CacheHitRate < 0.5 {
			suggestions = append(suggestions, models.CacheOptimizationSuggestion{
				Type:        "low_hit_rate",
				Pattern:     pattern,
				Priority:    "high",
				Description: "High frequency pattern with low cache hit rate",
				Recommendation: "Consider preloading or increasing TTL",
				Metrics: map[string]interface{}{
					"frequency":    stats.Frequency,
					"hit_rate":     stats.CacheHitRate,
					"avg_duration": stats.AvgDuration.Milliseconds(),
				},
			})
		}

		// Identify slow queries that could benefit from better caching
		if stats.AvgDuration > 2*time.Second && stats.Frequency > 5 {
			suggestions = append(suggestions, models.CacheOptimizationSuggestion{
				Type:        "slow_query",
				Pattern:     pattern,
				Priority:    "medium",
				Description: "Slow query with decent frequency",
				Recommendation: "Optimize query or implement background preloading",
				Metrics: map[string]interface{}{
					"frequency":    stats.Frequency,
					"avg_duration": stats.AvgDuration.Milliseconds(),
				},
			})
		}
	}

	return suggestions
}

// startAnalysis runs periodic analysis of search patterns
func (a *SearchCacheAnalyzer) startAnalysis() {
	ticker := time.NewTicker(a.analysisInterval)
	defer ticker.Stop()

	for range ticker.C {
		suggestions := a.GetOptimizationSuggestions()
		if len(suggestions) > 0 {
			// Log optimization suggestions
			// In a real implementation, this might trigger automated optimizations
		}
	}
}

// PreloadEngine manages intelligent cache preloading
type PreloadEngine struct {
	service      *enhancedSearchService
	patterns     []string
	batchSize    int
	interval     time.Duration
}

// NewPreloadEngine creates a new cache preload engine
func NewPreloadEngine(service *enhancedSearchService, patterns []string, batchSize int, interval time.Duration) *PreloadEngine {
	return &PreloadEngine{
		service:   service,
		patterns:  patterns,
		batchSize: batchSize,
		interval:  interval,
	}
}

// Start begins the preloading process
func (p *PreloadEngine) Start(ctx context.Context) {
	ticker := time.NewTicker(p.interval)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			p.executePreloading(ctx)
		}
	}
}

// executePreloading performs the actual preloading
func (p *PreloadEngine) executePreloading(ctx context.Context) {
	for _, pattern := range p.patterns {
		select {
		case <-ctx.Done():
			return
		default:
			if err := p.service.preloadSearchPattern(ctx, pattern); err != nil {
				// Log error but continue with other patterns
			}
		}
	}
}